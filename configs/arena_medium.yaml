ppo:
  steps: 25e6
  win_bonus: 2.0
  partial_score: 1.0
  gamma: 0.999
  seq_rosteps: 64
  num_envs: 128
  num_self_play: 64
adr:
  initial_hardness: 1.0
  max_hardness: 1.0
  hardness_offset: 1e15
  linear_hardness: true
task:
  objective: "ARENA_MEDIUM"
  task_hardness: 1
eval:
  eval_frequency: 500e3
  model_save_frequency: 1
  eval_envs: 128
  eval_timesteps: 1000
optimizer:
  lr: "step: 0.0005@0 cos 0.00005@25e6"
  vf_coef: 1.0
  entropy_bonus: "step: 0.5@0 lin 0.15@10e6 0.0@25e6"
  batch_size: 4096
  micro_batch_size: 2048
obs:
  allies: 8
  obs_enemies: 8
  obs_minerals: 8
  obs_map_tiles: 0
policy:
  agents: 8
  nenemy: 8
  nally: 8
  nmineral: 8
  ntile: 0