ppo:
  steps: 200e3
  seq_rosteps: 64
  num_envs: 128
  num_self_play: 64
task:
  objective: "ARENA_TINY"
  task_hardness: 1
eval:
  eval_frequency: 25e3
  model_save_frequency: 1
  eval_envs: 256
  eval_timesteps: 360
  eval_symmetric: False
optimizer:
  lr: "step: 0.003@0 lin 0.0@200e3"
  entropy_bonus: "step: 0.01@0 lin 0.0@200e3"
  vf_coef: 1.0
  batch_size: 2048
  micro_batch_size: 2048
obs:
  allies: 1
  obs_enemies: 1
  obs_minerals: 1
  obs_map_tiles: 0
policy:
  d_agent: 128
  agents: 1
  nenemy: 1
  nally: 1
  # Could be 0, currently incompatible with ally_enemy_same=False
  nmineral: 1
  ntile: 0
adr: {}